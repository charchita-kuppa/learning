# -*- coding: utf-8 -*-
"""Opinion spam detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jlrYr7KQmsiTRNGhqKiSA7cyegFdE2wH
"""

import nltk
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import wordnet
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import defaultdict
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords
import pandas as pd
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt
import string
import numpy as np
import re
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import FunctionTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import time

data = pd.read_csv('/content/sample_data/deceptive-opinion.csv')
data.head()
#print(data.shape)

"""Analyzing the data:"""

data.info()

data = data.rename(columns = {"deceptive":"label"})
data

data.describe()

"""The describe() method provides a summary statistics.There are 1600 labels and messages. There are two unique labels for deceptive and truthful. the number of unique messages (1596) out of total message count(1600) indicating there are 4 duplicated messages. The top label shows most frequently used in the data set """

data['textlength'] = data['text'].apply(len)
data

"""pre-processing the data:"""

# lower case all the reviews
for ind in data.index:
  sentence = data['text'][ind]
  sentence = sentence.lower()
  data['text'][ind] = sentence
data['text']

# remove punctuations and digits
def clean_data():
  regex = re.compile(r'[^\w\s]')
  data['text'] = data['text'].apply(lambda x: regex.sub('', x))
  regex = re.compile('\w*\d\w*')
  data['text'] = data['text'].apply(lambda x: regex.sub('', x))
clean_data()
data['text']

# remove stop words from the data
stop_words = stopwords.words('english')
string = ['bldg',"can't", 'cant', 'didnt','im', 'wasnt']
for word in string:
  stop_words.append(word)
for ind in data.index:
  word_tokenized = word_tokenize(data['text'][ind])
  word_sequence = [word for word in word_tokenized if not word in stop_words]
  data['text'][ind] = ' '.join(word_sequence)
data['text']

# lemmatization:

new_data = pd.DataFrame({'text' : []})
new_data['text'] = [word_tokenize(entry) for entry in data['text']]

lemmatizer = WordNetLemmatizer()
#get word net tags for the pos_tag(word) 
def getPosTag(tag_map):
  if tag_map.startswith('J'):
    return wordnet.ADJ
  elif tag_map.startswith('V'):
    return wordnet.VERB
  elif tag_map.startswith('N'):
    return wordnet.NOUN
  elif tag_map.startswith('R'):
    return wordnet.ADV
  else:
    return None 

# lemmetize the word according to its part of speech structure
for ind, word in new_data['text'].iteritems():
  lemmatizedWords = []
  for word,tag in pos_tag(word,tagset='universal'):
    wordNetTag = getPosTag(tag)
    if wordNetTag is None:
      lemmatizedWords.append(lemmatizer.lemmatize(word))
    else:
      lemmatizedWords.append(lemmatizer.lemmatize(word, pos=wordNetTag))
      new_data.loc[ind, 'text_final'] = str(lemmatizedWords)

data['text_final'] = new_data[['text_final']].copy()

data

"""vizualization of data:"""

# Get all the deceptive  and truthful  reviews
deceptive_reviews = data[data.label =='deceptive']
truthful_reviews = data[data.label =='truthful']

#frequency vs text length for deceptive reviews
plt.subplot(1,2,1)
plt.title('text length distribution of deceptive reviews')
plt.xlabel('textlength')
plt.ylabel('frequency')
deceptive_reviews['textlength'].hist(bins=100, figsize=(15,5), color='g')

#frequency vs text length for truthful reviews
plt.subplot(1,2,2)
plt.xlabel('textlength')
plt.ylabel('frequency')
plt.title('text length distribution of truthful reviews')
truthful_reviews['textlength'].hist(bins=100, figsize=(15,5), color='b',label='truthful reviews')


plt.show()

"""From the graph we can observe that text length for truthful reviews is more distributed than the text length for deceptive reviews."""

# Create numpy array to visualize using wordcloud
deceptive_reviews_text = " ".join(deceptive_reviews['text'].to_numpy())
truthful_reviews_text = " ".join(truthful_reviews['text'].to_numpy())

# wordcloud for deceptive reviews
deceptive_reviews_cloud = WordCloud(width =520, height =260, stopwords=None, max_font_size=100, max_words=50, background_color ="white", colormap='Greens').generate(deceptive_reviews_text)

# wordcloud for truthful reviews
truthful_reviews_cloud = WordCloud(width =520, height =260, stopwords=None, max_font_size=100,  max_words=50, background_color ="white", colormap='Greens').generate(truthful_reviews_text)

#plot word clouds
fig=plt.figure(figsize=(30,20))
ax1 = fig.add_subplot(2,2,1)
ax1.set_title('most frequently used words in deceptive reviews')
ax1.imshow(deceptive_reviews_cloud, interpolation='bilinear')
ax1.axis('off') # turn off axis

ax2 = fig.add_subplot(2,2,2)
ax2.set_title('most frequently used words in truthful reviews')
ax2.imshow(truthful_reviews_cloud, interpolation='bilinear')
ax2.axis('off') # turn off axis

plt.show()

"""The wordCloud() API gives most frquently used words with bigger font size. In these images above there are both unigrams and bigrams. If we observe both word clouds we can find the word 'price' has lerger font size in truthful reviews which means it is more frequently used in truthful reviews when compared to deceptive reviews similarly we could also find word 'lobby' more frquently used in truthful reviews than in deceptive reviews. The  next section gives more clarity about the more frequently unigrams and bigrams in deceptive and truthful individually"""

# get thefrequently used unigrams and bigrams for truthful and deceptive reviews
def getWordFrquency(data, n, range):
  vec=CountVectorizer(ngram_range=range).fit(data)
  word2Vec = vec.transform(data)
  sumOfWords = word2Vec.sum(axis=0) # count  number of times each word is repeated
  word_freq = [(word, sumOfWords[0, ind]) for word, ind in vec.vocabulary_.items()]
  word_freq = sorted(word_freq, key=lambda x : x[1], reverse=True)
  return word_freq[:n]

def createDataFrame(data):
  return pd.DataFrame(data, columns=['words', 'count'])

def plotwordsVsfrequency(dataframe):
  plt.xlabel('words')
  plt.ylabel('frequency')
  dataframe.groupby('words').sum()['count'].sort_values(ascending=False).plot(kind = 'bar')

#vizualizing unigrams for top 30 most used words in deceptive reviews
unigrams_deceptive = getWordFrquency(deceptive_reviews['text'], 30, (1,1))
deceptiveReviews = createDataFrame(unigrams_deceptive)

plt.figure(figsize=(20,5))
plt.subplot(1,2,1)
plt.title('unigrams for top 30 most used words in deceptive reviews')
plotwordsVsfrequency(deceptiveReviews)

#vizualizing unigrams for top 30 most used words in truthful reviews
unigrams_truthful = getWordFrquency(truthful_reviews['text'], 30, (1,1))
truthfulReviews = createDataFrame(unigrams_truthful)

plt.subplot(1,2,2)
plt.title('unigrams for top 30 most used words in truthful reviews') 
plotwordsVsfrequency(truthfulReviews)

plt.show()

"""Although the 'top 30 most used unigrams' for deceptive and truthful reviews are  somewhat same the number of times each word is used differ for both of them. For example the word 'chicago' is used 1000 times in deceptive reviews while it was used approximately around 400 to 600 times in truthful reviews"""

#vizualizing bigrams for top 30 most used words in deceptive reviews
bigrams_deceptive = getWordFrquency(deceptive_reviews['text'], 30, (2,2))
deceptiveReviews_bigrams = createDataFrame(bigrams_deceptive)

plt.figure(figsize=(20,5))
plt.subplot(1,2,1)
plt.title('bigrams for top 30 most used words in deceptive reviews')
plotwordsVsfrequency(deceptiveReviews_bigrams)

#vizualizing bigrams for top 30 most used words in truthful reviews
bigrams_truthful = getWordFrquency(truthful_reviews['text'], 30, (2,2))
truthfulReviews_bigrams = createDataFrame(bigrams_truthful)

plt.subplot(1,2,2) 
plt.title('bigrams for top 30 most used words in truthful reviews') 
plotwordsVsfrequency(truthfulReviews_bigrams)

plt.show()

"""If we observe the top 30 most used bigrams for deceptive and truthful reviews, the bigrams for deceptive reviews mostly contains hotel names like 'faimount chicago', 'conrad chicago', 'amalfi hotel', while the bigrams for truthful reviews contains information about street names like 'michigan avenue', about hotel staff like 'friendly helpful', 'hotel staff', 'desk staff', about location like 'location great', 'walking distance'.

Train and test data split:
"""

# encoding the labels 0 - deceptive, 1- truthful
target_labels = np.empty(1600)
for ind in data.index:
  if data['label'][ind] == 'deceptive':
    target_labels[ind] = 0
  else:
    target_labels[ind] = 1

# splitting the data for training and testing, 20% of data for testing and 80% for training
train_group, test_group, train_labels, test_labels =  train_test_split(data['text'], target_labels, test_size=0.20, train_size=0.80, random_state=101, shuffle=True)

"""Training the naive bayes classifier """

def classifierWithTextLength(features, range, string):
  
  def getTextLength(data):
    return np.array([len(text) for text in data]).reshape(-1, 1)

  #pipeline for TfIDF vectors for unigrams with max featurers 5000
  pipeline_text = Pipeline ([ ('vectorizer', CountVectorizer(max_features=features, ngram_range=range)),
                              ('tfidf', TfidfTransformer()) ])


  #pipeline for combined features (TFIDF vectors and text review length) and naive bayes classifier 
  classifier_NB = Pipeline([ ('features', FeatureUnion([
                                ('text',  pipeline_text),
                                ('length', FunctionTransformer(getTextLength, validate=False)),
                                ])),
                              ('classifier',MultinomialNB())])



  start_time = time.process_time()
  classifier_NB.fit(train_group, train_labels)
  trainingTime = time.process_time() - start_time 

  predicted_train = classifier_NB.predict(train_group)
  predicted_test = classifier_NB.predict(test_group)

  accuracy_Train = accuracy_score(predicted_train, train_labels)
  accuracy_Test  = accuracy_score(predicted_test, test_labels)

  if string == 'constant max_features':
    return trainingTime,accuracy_Train,accuracy_Test
  else:
    return accuracy_Train,accuracy_Test

def classifierWithoutTextLength(features, range, string):
  #pipeline for TfIDF vectors for with max featurers 5000 and naive bayes classifier
  classifier_NB = Pipeline([ ('vectors',CountVectorizer(max_features=features, ngram_range=range)),
                            ('TFIDF',TfidfTransformer()),
                            ('Naive Bayes Classifier',MultinomialNB()) ])

  start_time = time.process_time()
  classifier_NB.fit(train_group,train_labels)
  trainingTime = time.process_time() - start_time

  predictions_train = classifier_NB.predict(train_group)
  predictions_test = classifier_NB.predict(test_group)

  accuracy_Train = accuracy_score(predictions_train, train_labels)
  accuracy_Test  = accuracy_score(predictions_test, test_labels)
  
  if string == 'constant max_features':
    return trainingTime,accuracy_Train,accuracy_Test
  else:
    return accuracy_Train,accuracy_Test

#accuracy results using naive bayes classifier with Features: TFIDF vectors for unigrams with maximum features = 5000
print('Accuracy results : TFIDF vectors for unigrams')
trainingTime, accuracy_Train, accuracy_Test = classifierWithoutTextLength(4000, (1,1),'constant max_features')
print('Time taken in seconds to train naive bayes classifier with the data: %f' %(trainingTime))
print("Accuracy Score for train data -> ",accuracy_Train*100)
print("Accuracy Score for test data -> ",accuracy_Test*100)

# accuracy results using naive bayes classifier with Features: TFIDF vectors for unigrams with maximum features = 5000 and review text length
print('Accuracy results : TFIDF vectors for unigrams and text length')
trainingTime,accuracy_Train, accuracy_Test = classifierWithTextLength(4000,(1,1),'constant max_features')
print('Time taken in seconds to train naive bayes classifier with the data: %f' %(trainingTime))
print("Accuracy Score for train data -> ",accuracy_Train*100)
print("Accuracy Score for test data -> ",accuracy_Test*100)

#accuracy results using naive bayes classifier with Features: TFIDF vectors for bigrams with maximum features = 5000
print('Accuracy results : TFIDF vectors for bigrams')
trainingTime, accuracy_Train, accuracy_Test = classifierWithoutTextLength(4000, (2,2), 'constant max_features')
print('Time taken in seconds to train naive bayes classifier with the data: %f' %(trainingTime))
print("Accuracy Score for train data -> ",accuracy_Train*100)
print("Accuracy Score for test data -> ",accuracy_Test*100)

# accuracy results using naive bayes classifier with Features: TFIDF vectors for bigrams with maximum features = 5000 and review text length
print('Accuracy results : TFIDF vectors for bigrams and text length')
trainingTime, accuracy_Train, accuracy_Test = classifierWithTextLength(4000,(1,2),'constant max_features')
print('Time taken in seconds to train naive bayes classifier with the data: %f' %(trainingTime))
print("Accuracy Score for train data -> ",accuracy_Train*100)
print("Accuracy Score for test data -> ",accuracy_Test*100)

#accuracy results using naive bayes classifier with Features: TFIDF vectors for unigrams and bigrams with maximum features = 5000
print('Accuracy results : TFIDF vectors for unigrams and bigrams')
trainingTime,accuracy_Train, accuracy_Test = classifierWithoutTextLength(4000,(1,2),'constant max_features')
print('Time taken in seconds to train naive bayes classifier with the data: %f' %(trainingTime))
print("Accuracy Score for train data -> ",accuracy_Train*100)
print("Accuracy Score for test data -> ",accuracy_Test*100)

# accuracy results using naive bayes classifier with Features: TFIDF vectors for unigrams and bigrams with maximum features = 5000 and review text length
print('Accuracy results : TFIDF vectors for unigrams and bigrams and text length')
trainingTime,accuracy_Train, accuracy_Test = classifierWithTextLength(4000,(1,2),'constant max_features')
print('Time taken in seconds to train naive bayes classifier with the data: %f' %(trainingTime))
print("Accuracy Score for train data -> ",accuracy_Train*100)
print("Accuracy Score for test data -> ",accuracy_Test*100)

# accuracy results with unigram and bigram TFIDF vectors varying max_features from 500 to 6000
train_accuracy= np.empty(11)
test_accuracy = np.empty(11)
index = 0
for features in range(500, 6000, 500):
  train_accuracy[index], test_accuracy[index] = classifierWithTextLength(features, (1,2),'varying max_features')
  index = index+1

features = np.arange(500,6000,500)

plt.figure(figsize=(15,5))


plt.subplot(1,2,1)
plt.xlabel('number of features')
plt.ylabel('accuracy')

plt.title('training data')
plt.plot(features, train_accuracy)

plt.subplot(1,2,2)
plt.xlabel('number of features')
plt.ylabel('accuracy')

plt.title('testing data')
plt.plot(features, test_accuracy)

plt.show()

# accuracy results with unigram and bigram TFIDF vectors varying max_features from 500 to 6000 and 
train_accuracy= np.empty(11)
test_accuracy = np.empty(11)
index = 0
for features in range(500, 6000, 500):
  train_accuracy[index], test_accuracy[index] = classifierWithoutTextLength(features, (1,2),'varying max_features')
  index = index+1

features = np.arange(500, 6000, 500)

plt.figure(figsize=(15,5))


plt.subplot(1,2,1)
plt.xlabel('number of features')
plt.ylabel('accuracy')

plt.title('training data')
plt.plot(features, train_accuracy)

plt.subplot(1,2,2)
plt.xlabel('number of features')
plt.ylabel('accuracy')

plt.title('testing data')
plt.plot(features, test_accuracy)

plt.show()
